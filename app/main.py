import os
import dotenv
import google.generativeai as genai
import streamlit as st
import fitz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ==== FunÃ§Ãµes de Processamento de PDF ====
def extract_text_from_pdf(file):
    """Extrai texto de um arquivo PDF."""
    pdf_data = file.read()
    doc = fitz.open(stream=pdf_data, filetype="pdf")
    text = ""
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text += page.get_text()
    return text

def split_text(text, max_len=500):
    """Divide o texto em segmentos menores."""
    words = text.split()
    segments = []
    current_segment = []
    current_length = 0
    for word in words:
        if current_length + len(word) + 1 > max_len:
            segments.append(' '.join(current_segment))
            current_segment = []
            current_length = 0
        current_segment.append(word)
        current_length += len(word) + 1
    if current_segment:
        segments.append(' '.join(current_segment))
    return segments

def create_embeddings(all_text_segments, client):
    """Cria embeddings dos segmentos de texto usando TF-IDF."""
    vectorizer = TfidfVectorizer()
    embeddings = vectorizer.fit_transform(all_text_segments)
    return vectorizer, embeddings

def search_with_embeddings(prompt, vectorizer, embeddings, all_text_segments):
    """Busca o segmento mais relevante com base nos embeddings."""
    query_vec = vectorizer.transform([prompt])
    similarities = cosine_similarity(query_vec, embeddings).flatten()
    best_match_index = np.argmax(similarities)
    best_match_segment = all_text_segments[best_match_index]
    return best_match_index, best_match_segment

def load_existing_pdfs():
    pdf_paths = [os.path.join("./Data", filename) 
                 for filename in os.listdir("./Data") 
                 if filename.lower().endswith(".pdf")] 
    all_text_segments = []
    for pdf_path in pdf_paths:
        with open(pdf_path, "rb") as file:
            text = extract_text_from_pdf(file)
            text_segments = split_text(text)
            all_text_segments.extend(text_segments)
    return all_text_segments


# ==== ConfiguraÃ§Ã£o da API do Gemini Pro ====
dotenv.load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-pro")


# ==== Carregamento de PDFs Existentes (agora em uma funÃ§Ã£o) ====
all_text_segments = load_existing_pdfs()
vectorizer, embeddings = create_embeddings(all_text_segments, client=None)

# ==== Barra Lateral Personalizada ====
with st.sidebar:
    st.markdown(
        """
        <div style="text-align: center;">
            <img src="https://i.imgur.com/10PJpQH.png?lm=1668947335" width="150">
            <h1>Alpha-C, ao seu dispor! ðŸ’¬</h1>
        </div>
        """, unsafe_allow_html=True)

    # ==== BotÃ£o de Upload de Arquivo ====
    uploaded_file = st.file_uploader("Upload ðŸ“Ž:", type=["pdf"])
    if uploaded_file is not None:
        original_filename = uploaded_file.name
        name, ext = os.path.splitext(original_filename)
        ext = ext.lower()
        save_path = os.path.join("./Data", name + ext)
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getvalue())
        st.success(f"Arquivo PDF salvo com sucesso em: {save_path}")

        all_text_segments = load_existing_pdfs()
        vectorizer, embeddings = create_embeddings(all_text_segments, client=None)
        st.success("Embeddings criados com sucesso!")


# ==== Chat e Processamento Principal ====
st.markdown(
    """
    <div style="text-align: center;">
        <img src="https://i.imgur.com/LlUM5am.jpg?lm=1668947335" width="150">
        <h1>IA Generativa PI</h1>
    </div>
    """, unsafe_allow_html=True)

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "OlÃ¡! Alpha-C na Ã¡rea, o seu assistente pessoal da Programmers Beyond IT, aqui pra te dar um suporte de primeira. ðŸ˜Ž\n\nDiga como posso lhe ajudar, qual informaÃ§Ã£o esta em busca? Estou a disposiÃ§Ã£o pra te ajudar! ðŸ‘‹\n\n"}
    ]

avatar = 'https://i.imgur.com/10PJpQH.png?lm=1668947335'
for msg in st.session_state.messages:
    st.chat_message(msg["role"], avatar=avatar if msg["role"] == "assistant" else None).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)  # Removido o avatar do usuÃ¡rio

    if prompt:
        try:
            best_match_index, best_match_segment = search_with_embeddings(prompt, vectorizer, embeddings, all_text_segments)

            chat_session = model.start_chat(
                history=[
                    {
                        "role": "model",
                        "parts": [
                            "VocÃª Ã© um assistente pessoal muito ligeiro, fala em girias e seu nome Ã© Alpha-C. Sua funÃ§Ã£o Ã© servir a todos os colaboradores da Programmers Beyond IT.",
                            "NÃ£o responda perguntas que fujam do tema: Programmers Beyond IT. E nÃ£o invente respostas",
                            "VocÃª nÃ£o deve desencorajar o usuÃ¡rio. Sempre forneÃ§a informaÃ§Ãµes Ãºteis e positivas",
                        ],
                    },
                    {
                        "role": "user",
                        "parts": [
                            f"Use o seguinte contexto para responder: {best_match_segment}",
                            prompt
                        ],
                    }
                ]
            )

            response = chat_session.send_message(prompt)
            answer = response.text

            st.session_state["messages"].append({"role": "assistant", "content": answer})
            st.chat_message("assistant", avatar=avatar).write(answer)
        except Exception as e:
            st.error(f"Gemini API error: {e}")
